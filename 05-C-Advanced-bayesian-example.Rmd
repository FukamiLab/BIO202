---
layout: topic
title: "Advanced Bayesian model example"
author: Jes, Lizzie
output: html_document
---

**Assigned Reading:**

> Section 14.2 in Korner-Nievergelt et al. 2015. *Bayesian data analysis in ecology using linear models with R, BUGS, and Stan.* Elsevier. [link](http://www.sciencedirect.com/science/book/9780128013700) 

**Optional Reading:**

> Chapters 16 and 17 in Korner-Nievergelt et al. 2015. *Bayesian data analysis in ecology using linear models with R, BUGS, and Stan.* Elsevier. [link](http://www.sciencedirect.com/science/book/9780128013700)
>
> \* These chapters contain a data analysis checklist and information on how to report analyses in scientific papers. Chapter 17 is particularly useful because it gives verbatim text for how you should describe the models that you fit.

```{r include = FALSE}
# This code block sets up the r session when the page is rendered to html
# include = FALSE means that it will not be included in the html document

# Write every code block to the html document 
knitr::opts_chunk$set(echo = TRUE)

# Write the results of every code block to the html document 
knitr::opts_chunk$set(eval = TRUE)

# Define the directory where images generated by knit will be saved
knitr::opts_chunk$set(fig.path = "images/05-C/")

# Set the web address where R will look for files from this repository
# Do not change this address
repo_url <- "https://raw.githubusercontent.com/fukamilab/BIO202/master/"
```

### Key Points

We want to fit a zero-inflated Poisson mixed-effects model, but the `hurdle()` and `zeroinfl()` functions from the `pscl` package don't allow random effects. A Bayesian approach is more flexible and will allow us to fit this model.

**The Model**

Today's example is a zero-inflated Poisson model with a random intercept in the portion of the model controlling zero-inflation. This example is taken directly from Section 14.2 of Korner-Nievergelt et al. (2015). (See above citation.) 

In the following model, the subscript $i$ denotes a specific nestID and the subscript $t$ denotes a specific sampling year. Bold variables are vectors.


Create an unobserved (latent) variable $\mathbf{z}$ that equals 1 with probability $\boldsymbol{\theta}$.

$$z_{it} \sim Bernoulli(\theta_{it})$$

If $\mathbf{z}$ is 1, then the observed variable $\mathbf{y}$ is 0. Otherwise, $\mathbf{y}$ is Poisson distributed with mean $\boldsymbol{\lambda}$ (I.e. This is a mixture model because $\mathbf{y}$ can be 0 for two different reasons.)

$$y_{it} \sim Poisson(\lambda_{it}(1 - z_{it}))$$

The log odds that $\mathbf{z}$ is 1 depends linearly on year ($a_2$) and different nestIDs are allowed to have different effects ($\boldsymbol{\epsilon_{nestID}}$) on the overall mean ($a_1$) 

$$logit(\theta_{it}) = a_1 + a_2 year_t + \epsilon_{nestID[i]}$$

However, the effects of different nestIDs are constrained to come from a Normal distribution with mean $0$ and variance $\sigma_n^2$. Hence they are a "random" effect.

$$\epsilon_{nestID} \sim Norm(0, \sigma_n)$$

If $\mathbf{z}$ is $0$, then (the log of) the expected value of $\mathbf{y}$ depends linearly on year ($b_2$).

$$log(\lambda_{it}) = b_1 + b_2 year_t$$









### Analysis Example

In review, the zero-inflated Poisson model is a mixture of binomial and a Poisson distribution. Like all mixed models, it addresses data generated by multiple processes, as seen in life sciences, and the distributions that result from such data. Neglecting these different processes could yield biased inferences. Using multi-level models are especially useful to help reveal the different processes underlying our data. 

Count data typically contains a high proportion of zeros and varying data, as in our example of black-stork nestlings. The number of surviving nestlings is often zero due to depradation or natural events. Furthermore, in nests that succeed, numbers of surviving nestlings differ greatly across nests. Hence, these count data (or zero-inflated data) result from two distinct processes, one producing zero counts and one producing non-zero count variance. Recall the bimodal histogram of zero-inflated data, with one peak at zero and another at a greater value. To address zero-laden bimodal distributions, zero-inflated models blend a Bernoulli model (zero vs. nonzero) with a conditional Poisson model--conditional on the Bernoulli process being nonzero. To fit zero-inflated models, we use the function `zeroinfl` from the package pscl.

The question posed in our example is: Did the number of black-stork nestlings surviving in Latvia decline over time? The authors use a zero-inflated Poisson model to estimate temporal trends in nest survival and, separately, the number of surviving nestlings in extant nests. The nests were repeatedly measured over 17 years.

<br>

**First steps: Read and prepare data for STAN** 
Example data: Breeding success of Black-stork in Latvia. The data were collected and provided by Maris Stradz. They contain the number of nestlings of more than 300 black-stork nests in different years.

*Read data*

```{r include = TRUE}
library(blmeco)
data(blackstork)
dat <- blackstork

# define nest as a factor 
dat$nest <- factor(dat$nest)
# z-transform year
dat$year.z <- as.numeric(scale(dat$year))

```

*Prepare data for Stan*

```{r include = TRUE}
y <- dat$njuvs
N <- nrow(dat)
nest <- as.numeric(dat$nest)
Nnests <- nlevels(dat$nest)
year <- dat$year.z
datax <- c("y", "N", "nest", "Nnests", "year")

```

<br>

**Run STAN**

Next, we load the Stan code from the text file. In the book, the authors use the function `log_sum_exp` to calculate the density function of the zero count. The function `increment_log_prob` defines likelihood in Stan; however, the updated Stan language uses `target += u;` instead. Functions `bernoulli_log` and `poisson_log` define the density functions' logarithms; they've also been updated to `bernoulli_lpdf` and `poisson_lpdf`.

There appeared to be an issue with the original code in the book, namely with "sigmanest." I changed it to "sigma" to solve the issue.

Once you run the stan() function, save the model object to an RData file. Then you can load the fitted model and samples without having to run the stan() function every time you re-open the R script: load("code/05-C-stanmod.RData").


```{r include = TRUE}
library(rstan)
# Fit the model
# mod <- stan(file = "zeroinfl.stan", data=datax, chains=3, iter=1000)

# Save the model output
# save(mod, file = "05-C-stanmod.RData")

# Re-load the model output
load("05-C-stanmod.RData")

# View the 
print(mod, pars = c("a", "b", "sigma"))
```

The `mean` in the first column is the average of the simulated values for each parameter's marginal posterior distribution. `se_mean` refers to the standard error of the simulated values (Monte Carlo uncertainty). `sd` is the simulations' sample standard deviation, corresponding to the parameter estimates' standard error.

*Look at convergence*

```{r include = TRUE}
traceplot(mod, "a")
traceplot(mod, "b")
traceplot(mod, "sigma")
```

*Draw plots of the parameter estimates*

```{r include = TRUE}
plot(mod)
```

<br>

**Predictive model checking**

Before interpreting results, we need to assess the model fit using predictive model checking. Accordingly, we first simulate replicated numbers of nestlings for every year and nest in the data set per the model fit.

*Extract number of simulations*

```{r include = TRUE}
modsims <- extract(mod)
nsim <- length(modsims$lp_)
yrep <- matrix(nrow=length(y), ncol=nsim)
Xmat <- model.matrix(~year)
for(i in 1:nsim){
  theta <- plogis(Xmat%*%modsims$a[i,] + 
                    modsims$groupefftheta[i,nest])
  z <- rbinom(length(y), prob=theta, size=1)
  lambda <- (1-z) *exp(Xmat%*%modsims$b[i,])
  yrep[,i] <- rpois(length(y), lambda=lambda)
}
```

*Sort the nests according to the first year for the plot*

```{r include = TRUE}
first <- tapply(year, nest, min)
first <- first[match(nest, 1:Nnests)]
nestnamenum <- first*1000 + nest
nestnamenum <- as.numeric(factor(nestnamenum))
```

*Visualize temporal pattern of zeros and non-zeros (Figure 14.3)*

```{r include = TRUE}

par(mfrow=c(1,5), mar=c(0,0,2,0.1), oma=c(2,2,0,0))
# First plot the real data
plot(year, nestnamenum,  col=rgb(1,0,0,0.5), pch=16, cex=dat$njuv/5, axes=FALSE, main="observed")
points(year[dat$njuv==0], nestnamenum[dat$njuv==0], cex=0.3) # add zeros
# Add data from 4 simulated data-sets 
for(r in 1:4){
  plot(year, nestnamenum, col=rgb(1,0,0,0.5), pch=16, cex=yrep[,r]/5, axes=FALSE, main="replicated")
  points(year[yrep[,r]==0], nestnamenum[yrep[,r]==0], cex=0.3) # add zeros 
}
mtext("year", outer=TRUE, side=1)
mtext("nest", outer=TRUE, side=2)

```


*Proportion of zeros for the observed data*

```{r include = TRUE}
mean(y==0)
```

*Proportion of zeros for the nsim simulated data*

```{r include = TRUE}
propzeros <- apply(yrep, 2, function(x) mean(x==0)) # 
quantile(propzeros, prob=c(0.025, 0.5, 0.975))
```


*Examine results*

```{r include = TRUE}
apply(modsims$a, 2, quantile, prob=c(0.025, 0.5, 0.957))
```
Recall that `a` represents number of zero-survivor nests. Hence, the positive slope indicates that the number of failed nests increased over the years.

```{r include = TRUE}
apply(modsims$b, 2, quantile, prob=c(0.025, 0.5, 0.957))
```
And the number of nestlings in successful nests (indicated by `b`) decreased over time, per the estimated slope of the Poisson model's regression coefficient (-0.05).

<br>

**Visualize results**

Three regression lines will help us understand the results: (1) the proportion of successful nests, (2) the number of nestlings in successful nests, (3) the mean number of nestlings. To draw these lines, we need to create a new data frame with predictor variable "year." Before fitting this model, we transform this variable and create a new matrix.

We then extract the posterior distributions' means for the Bernoulli coefficients ("ahat") and Poisson model ("bhat"). To calculate the estimated proportions of surviving nests, we subtract the Bernoulli model's fitted values from 1. (Recall that `zit = 1` denotes a dead nest.) Add the fitted value from the Poisson model, the average number of successful nests' nestlings. We multiply the fraction of surviving nests by the mean number of nestlings for successful nests to yield the average number of nestlings per year, averaged over suriving and failed nests.

```{r include = TRUE}
newdat <- data.frame(year=1979:2010)
newdat$year.z <- (newdat$year - mean(dat$year))/sd(dat$year)
Xmat <- model.matrix(~year.z, data=newdat)
ahat <- apply(modsims$a, 2, mean)            # extract the estimates for a
bhat <- apply(modsims$b, 2, mean)            # extract the estimates for b
newdat$propsfit <- 1-plogis(Xmat%*%ahat)     # Proportion of nests that survived
newdat$nnestfit <- exp(Xmat%*%bhat) 
newdat$avnnestfit<-newdat$propsfit*newdat$nnestfit
```

Using `nsim`, we can repeat the preceding calculations with each set of model parameters--a novel MCMC iteration. And we bound every fitted value using the 2.5% and 97.5% quantiles as the lower and upper limits of a 95% CrI.

```{r include = TRUE}
nsim <- length(modsims$lp_)
propsmat <- matrix(ncol=nsim, nrow=nrow(newdat))
nnestmat <- matrix(ncol=nsim, nrow=nrow(newdat))
avnnestmat <- matrix(ncol=nsim, nrow=nrow(newdat))
for(i in 1:nsim){
  propsmat[,i] <- 1-plogis(Xmat%*%modsims$a[i,])
  nnestmat[,i] <- exp(Xmat%*%modsims$b[i,])
  avnnestmat[,i] <- propsmat[,i]*nnestmat[,i]
}
newdat$propslwr <- apply(propsmat, 1, quantile, prob=0.025)
newdat$propsupr <- apply(propsmat, 1, quantile, prob=0.975)
newdat$nnestlwr <- apply(nnestmat, 1, quantile, prob=0.025)
newdat$nnestupr <- apply(nnestmat, 1, quantile, prob=0.975)
newdat$avnnestfit <- newdat$propsfit*newdat$nnestfit
newdat$avnnestlwr <- apply(avnnestmat, 1, quantile, prob=0.025)
newdat$avnnestupr <- apply(avnnestmat, 1, quantile, prob=0.975)
```


*To visualize the regression lines (Figure 14.4)*

```{r include = TRUE}

par(mfrow=c(3,1), mar=c(1.5,5.5,0.1,1), oma=c(3,0,0,0))

# a) Number of nestlings
plot(dat$year, dat$njuvs, xlab="Year", type="n", las=1,
     ylab="Number of nestlings", xaxt="n",
     cex.axis=1.2, cex.lab=1.4, ylim=c(-0.4, 5.4))
polygon(c(newdat$year, rev(newdat$year)), 
        c(newdat$avnnestlwr, rev(newdat$avnnestupr)), 
        border=NA, col=grey(0.5))
lines(newdat$year, newdat$avnnestfit, lwd=2)
points(jitter(dat$year), jitter(dat$njuvs), cex=0.5)

# b) Number of Nestlings in successful nests
plot(dat$year, dat$njuvs, xlab="Year", type="n",las=1,
     ylab="Number of nestlings\nin successful nests", xaxt="n",
     cex.axis=1.2, cex.lab=1.4)
polygon(c(newdat$year, rev(newdat$year)), c(newdat$nnestlwr, 
                                            rev(newdat$nnestupr)), 
        border=NA, col=grey(0.5))
lines(newdat$year, newdat$nnestfit, lwd=2)

# c) Proportion of successful nests
plot(dat$year, seq(0,1, length=nrow(dat)), xlab="Year", type="n",
     ylab="Proportion of\nsuccessful nests",las=1,
     cex.axis=1.2, cex.lab=1.4)
polygon(c(newdat$year, rev(newdat$year)), c(newdat$propslwr, 
                                            rev(newdat$propsupr)), 
        border=NA, col=grey(0.5))
lines(newdat$year, newdat$propsfit, lwd=2)
mtext("Year", outer=TRUE, side=1, line=2, cex=1.2)


```

<br>

### Discussion Questions

+ Cold pizza from Nicole's discussion Wednesday: What's the golden rule when it comes to choosing optimal priors? They should not be (1) too strong, so that they are not providing us with additional information (not found in the dataset - too much overlap between prior and posterior), but also not too weak (2) to not influence the results due to lack of prior knowledge. Is there a better way than exploring different options via trail and error?
+ What are potential shortcomings of using the zero-inflated model, for instance, in the case of having a high correlation of the model parameters between the zero model and the count model? How do you address such problems?
+ What other information would be helpful to validate your zero-inflated model?
+ What's it all about, Alfie?




